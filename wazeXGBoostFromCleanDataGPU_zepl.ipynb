{
  "metadata": {
    "name": "wazeETLforXGBoost",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalgual/dalgual.github.io/blob/main/wazeXGBoostFromCleanDataGPU_zepl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmIj2FnvzDKs"
      },
      "source": [
        "# Waze ETL for XGBoost Spark with GPU\n",
        "\n",
        "Mortgage is an example of xgboost classifier to do binary classification. This notebook will show you how to load data, train the xgboost model and use this model to predict if a mushroom is \"poisonous\". Camparing to original XGBoost Spark code, there're only one API difference.\n",
        "\n",
        "## Load libraries\n",
        "First load some common libraries will be used by both GPU version and CPU version xgboost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "hvoGdl8szDKv"
      },
      "outputs": [],
      "source": [
        "%spark\n",
        "import ml.dmlc.xgboost4j.scala.spark.{XGBoostClassifier, XGBoostClassificationModel}\n",
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
        "import org.apache.spark.sql.types.{DoubleType, IntegerType, StructField, StructType}\n",
        "import org.apache.spark.SparkConf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "AH0gvhp7zDK4"
      },
      "outputs": [],
      "source": [
        "%spark\n",
        "import  org.apache.spark.sql.types\n",
        "import  org.apache.spark.sql.functions\n",
        "\n",
        "import org.apache.spark.ml.Pipeline\n",
        "import org.apache.spark.ml.classification.DecisionTreeClassifier\n",
        "import org.apache.spark.ml.classification.RandomForestClassifier \n",
        "import  org.apache.spark.ml.feature.{StringIndexer, VectorIndexer, MinMaxScaler, SQLTransformer, Normalizer}\n",
        "import  org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit, CrossValidator}\n",
        "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
        "\n",
        "import org.apache.spark.storage.StorageLevel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oydc0IBVzDLE"
      },
      "source": [
        "Besides CPU version requires some extra libraries, such as:\n",
        "\n",
        "```scala\n",
        "import org.apache.spark.ml.feature.VectorAssembler\n",
        "import org.apache.spark.sql.DataFrame\n",
        "import org.apache.spark.sql.functions._\n",
        "import org.apache.spark.sql.types.FloatType\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmZJDdQ1zDLE"
      },
      "source": [
        "## Build the schema and parameters\n",
        "The mortgage data has 27 columns: 26 features and 1 label. \"deinquency_12\" is the label column. The schema will be used to load data in the future.\n",
        "\n",
        "The next block also defines some key parameters used in xgboost training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "9_wZjz1ZzDLE"
      },
      "outputs": [],
      "source": [
        "\n",
        "val IS_CPU = false\n",
        "\n",
        "val IS_40M = true\n",
        "val IS_FULL = true\n",
        "\n",
        "val IS_HDFS = true\n",
        "\n",
        "var folder_name = \"gb1.6.parquet\" //\"gb0.8.parquet\" //\"gb400m.parquet\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "IHJ27O5UzDLF"
      },
      "outputs": [],
      "source": [
        "// Build the spark session and data reader as usual\n",
        "val conf = new SparkConf()\n",
        "conf.set(\"spark.executor.instances\", \"15\") // \"\"20\")\n",
        "conf.set(\"spark.executor.cores\", \"7\") // 7 \n",
        "conf.set(\"spark.task.cpus\", \"7\") // 4 but cannot change this here; The number of cores per executor (=4) has to be >= the task config: spark.task.cpus = 7 when run on yarn\n",
        "conf.set(\"spark.executor.memory\", \"14g\") //24g\n",
        "conf.set(\"spark.rapids.memory.pinnedPool.size\", \"2G\")\n",
        "conf.set(\"spark.executor.memoryOverhead\", \"16G\")\n",
        "conf.set(\"spark.executor.extraJavaOptions\", \"-Dai.rapids.cudf.prefer-pinned=true\")\n",
        "conf.set(\"spark.locality.wait\", \"0s\")\n",
        "conf.set(\"spark.sql.files.maxPartitionBytes\", \"512m\")\n",
        "conf.set(\"spark.executor.resource.gpu.amount\", \"1\")\n",
        "conf.set(\"spark.task.resource.gpu.amount\", \"1\")\n",
        "conf.set(\"spark.plugins\", \"com.nvidia.spark.SQLPlugin\")\n",
        "conf.set(\"spark.rapids.sql.hasNans\", \"false\")\n",
        "conf.set(\"spark.rapids.sql.batchSizeBytes\", \"512M\")\n",
        "conf.set(\"spark.rapids.sql.reader.batchSizeBytes\", \"768M\")\n",
        "conf.set(\"spark.rapids.sql.variableFloatAgg.enabled\", \"true\")\n",
        "conf.set(\"spark.rapids.memory.gpu.pooling.enabled\", \"false\")\n",
        "// conf.set(\"spark.rapids.memory.gpu.allocFraction\", \"0.1\")\n",
        "val spark = SparkSession.builder.appName(\"waze-gpu\")\n",
        "                               .enableHiveSupport()\n",
        "                               .config(conf)\n",
        "                               .getOrCreate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "y3gXJZuezDLG"
      },
      "outputs": [],
      "source": [
        "\n",
        "val labelColName = \"trueLabel\"\n",
        "\n",
        "val schema = StructType(List(\n",
        "  StructField(\"location_x\", DoubleType),\n",
        "  StructField(\"location_y\", DoubleType),\n",
        "  StructField(\"sin_weekday\", DoubleType),\n",
        "  StructField(\"cos_weekday\", DoubleType),\n",
        "  StructField(\"sin_month\", DoubleType),\n",
        "  StructField(\"cos_month\", DoubleType),\n",
        "  StructField(\"sin_day\", DoubleType), \n",
        "  StructField(\"cos_day\", DoubleType),\n",
        "  StructField(\"sin_hour\", DoubleType),\n",
        "  StructField(\"cos_hour\", DoubleType),\n",
        "  StructField(\"sin_min\", DoubleType),\n",
        "  StructField(\"cos_min\", DoubleType),\n",
        "  StructField(\"sin_sec\", DoubleType),\n",
        "  StructField(\"cos_sec\", DoubleType),\n",
        "  StructField(\"is_rush\", IntegerType),\n",
        "  StructField(\"is_weekend\", IntegerType),\n",
        "  StructField(\"is_holiday\", IntegerType),\n",
        "  StructField(\"level\", IntegerType),\n",
        "  StructField(labelColName, IntegerType)))\n",
        "\n",
        "val featureNames = schema.filter(_.name != labelColName).map(_.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udRoisLXzDLG"
      },
      "source": [
        "## Create a new spark session and load data\n",
        "\n",
        "A new spark session should be created to continue all the following spark operations.\n",
        "\n",
        "NOTE: in this notebook, the dependency jars have been loaded when installing toree kernel. Alternatively the jars can be loaded into notebook by [%AddJar magic](https://toree.incubator.apache.org/docs/current/user/faq/). However, there's one restriction for `%AddJar`: the jar uploaded can only be available when `AddJar` is called just after a new spark session is created. Do it as below:\n",
        "\n",
        "```scala\n",
        "import org.apache.spark.sql.SparkSession\n",
        "val spark = SparkSession.builder().appName(\"mortgage-GPU\").getOrCreate\n",
        "%AddJar file:/data/libs/cudf-XXX-cuda10.jar\n",
        "%AddJar file:/data/libs/rapids-4-spark-XXX.jar\n",
        "%AddJar file:/data/libs/xgboost4j_3.0-XXX.jar\n",
        "%AddJar file:/data/libs/xgboost4j-spark_3.0-XXX.jar\n",
        "// ...\n",
        "```\n",
        "\n",
        "##### Please note the new jar \"rapids-4-spark-XXX.jar\" is only needed for GPU version, you can not add it to dependence list for CPU version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "qFG8fBImzDLH"
      },
      "outputs": [],
      "source": [
        "// null error because of total 2+26 columns: remove product_type from above schema\n",
        "//val reader = spark.read.option(\"header\", true).schema(schema)\n",
        "val reader = spark.read.option(\"header\", true) //.schema(schema)\n",
        "\n",
        "// total 2+25 columns & fit() error below: product_type does not exist\n",
        "// val reader = spark.read.option(\"header\", true).option(\"inferSchema\" , \"true\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "U-P6QPHzzDLH"
      },
      "outputs": [],
      "source": [
        "val modelPath = \"gs://hdp-240/waze/model/\"\n",
        "var file_location = \"gs://hdp-240/waze/\" + folder_name\n",
        "\n",
        "/*\n",
        "if (IS_40M){\n",
        "    file_location = \"gs://hdp-240/waze/gb400m.parquet\"\n",
        "    // val file_location = \"gs://hdp-240/waze/gb40m.parquet\"\n",
        "} else {\n",
        "    //val file_location = \"/user/jwoo5/waze/gb1.6.parquet\"\n",
        "    //file_location = \"gs://hdp-240/waze/gb1.6.parquet\"\n",
        "    file_location = \"gs://hdp-240/waze/gb0.8.parquet\"\n",
        "}\n",
        "*/\n",
        "val entireData = reader.parquet(file_location)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "_0XECoAPzDLH"
      },
      "outputs": [],
      "source": [
        "val temp_table_name = \"jampredictclean_1m_100mb_csv\"\n",
        "entireData.createOrReplaceTempView(temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "hLBo7eEqzDLH"
      },
      "outputs": [],
      "source": [
        "entireData.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "dlq-S9xyzDLI"
      },
      "outputs": [],
      "source": [
        "//entireData.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "K4KclKeRzDLI"
      },
      "outputs": [],
      "source": [
        "/*\n",
        "val jam_int_weekday = spark.sql(\"\"\"UPDATE jampredictclean_1m_100mb_csv SET weekday =\n",
        "  |CASE \n",
        "  |    WHEN weekday = 'Monday' THEN 0 \n",
        "  |    WHEN weekday = 'Tuesday' THEN 1\n",
        "  |    WHEN weekday = 'Wednesday' THEN 2 \n",
        "  |    WHEN weekday = 'Thursday' THEN 3 \n",
        "  |    WHEN weekday = 'Friday' THEN 4 \n",
        "  |    WHEN weekday = 'Saturday' THEN 5 \n",
        "  |    WHEN weekday = 'Sunday' THEN 6 \n",
        "  |END\n",
        " |FROM jampredictclean_1m_100mb_csv\"\"\")\n",
        " */\n",
        " \n",
        " val jam_int_weekday = entireData.withColumn(\"weekday\", when(col(\"weekday\") === \"Monday\",\"0\")\n",
        "      .when(col(\"weekday\") === \"Tuesday\",\"1\")\n",
        "      .when(col(\"weekday\") === \"Wednesday\",\"2\")\n",
        "      .when(col(\"weekday\") === \"Thursday\",\"3\")\n",
        "      .when(col(\"weekday\") === \"Friday\",\"4\")\n",
        "      .when(col(\"weekday\") === \"Saturday\",\"5\")\n",
        "      .when(col(\"weekday\") === \"Sunday\",\"6\"))\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "U_q_IIWkzDLI"
      },
      "outputs": [],
      "source": [
        "//jam_int_weekday.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "OL9UlOngzDLJ"
      },
      "outputs": [],
      "source": [
        "val jam_temp_table_name = \"jam_int_weekday\"\n",
        "jam_int_weekday.createOrReplaceTempView(jam_temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "tWUwEclZzDLJ"
      },
      "outputs": [],
      "source": [
        "val holidays = spark.read.option(\"header\", true).option(\"inferSchema\" , true).csv(\"gs://hdp-240/waze/holidays_2018.csv\")\n",
        "//val holidays = spark.read.csv(\"gs://hdp-240/waze/holidays_2018.csv\", inferSchema=True, header=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "pigfN2yVzDLJ"
      },
      "outputs": [],
      "source": [
        "val joinCsv = entireData.join(holidays, (entireData(\"month\") === holidays(\"month\")) && (entireData(\"day\") === holidays(\"day\")), \"left\").select(entireData(\"location_x\"), entireData(\"location_y\"),entireData(\"pub_millis\"),  entireData(\"level\"), entireData(\"speed\"), entireData(\"pub_date\"), entireData(\"date_pst\"), entireData(\"month\"), entireData(\"day\"), entireData(\"hour\"), entireData(\"min\"), entireData(\"sec\"), entireData(\"weekday\"), holidays(\"Comments\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "ruYUpUGRzDLK"
      },
      "outputs": [],
      "source": [
        "// Create a view or table: jwoo5\n",
        "val temp_table_name_holy = \"holidays_2018_csv\"\n",
        "joinCsv.createOrReplaceTempView(temp_table_name_holy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "E8WMqX2GzDLL"
      },
      "outputs": [],
      "source": [
        "joinCsv.show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "U_gEmsXAzDLL"
      },
      "outputs": [],
      "source": [
        "/*\n",
        "val holy_int_weekday = spark.sql(\"\"\"UPDATE t2 SET \n",
        "  |CASE \n",
        "  |    WHEN t2.weekday = 'Monday' THEN 0 \n",
        "  |    WHEN t2.weekday = 'Tuesday' THEN 1\n",
        "  |    WHEN t2.weekday = 'Wednesday' THEN 2 \n",
        "  |    WHEN t2.weekday = 'Thursday' THEN 3 \n",
        "  |    WHEN t2.weekday = 'Friday' THEN 4 \n",
        "  |    WHEN t2.weekday = 'Saturday' THEN 5 \n",
        "  |    WHEN t2.weekday = 'Sunday' THEN 6 \n",
        "  |END AS weekday, \n",
        " |FROM holidays_2018_csv t2\"\"\")\n",
        " */\n",
        " \n",
        "  val holy_int_weekday = joinCsv.withColumn(\"weekday\", when(col(\"weekday\") === \"Monday\",\"0\")\n",
        "      .when(col(\"weekday\") === \"Tuesday\",\"1\")\n",
        "      .when(col(\"weekday\") === \"Wednesday\",\"2\")\n",
        "      .when(col(\"weekday\") === \"Thursday\",\"3\")\n",
        "      .when(col(\"weekday\") === \"Friday\",\"4\")\n",
        "      .when(col(\"weekday\") === \"Saturday\",\"5\")\n",
        "      .when(col(\"weekday\") === \"Sunday\",\"6\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "nIJM_PlpzDLL"
      },
      "outputs": [],
      "source": [
        "holy_int_weekday.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "fLY_Q1ODzDLM"
      },
      "outputs": [],
      "source": [
        "val holy_temp_table_name = \"holy_int_weekday\"\n",
        "holy_int_weekday.createOrReplaceTempView(holy_temp_table_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "n3ND7eBnzDLM"
      },
      "outputs": [],
      "source": [
        "//holy_int_weekday.show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "UnrDjZeszDLM"
      },
      "outputs": [],
      "source": [
        "/*\n",
        "val csv = spark.sql(\"\"\"SELECT t1.location_x, t1.location_y, t1.pub_millis, t1.level, t1.speed, t1.pub_date, t1.date_pst, t1.month, t1.day, t1.hour, t1.min, t1.sec, \n",
        "  |CASE \n",
        "  |    WHEN t1.month = t2.month and t1.day = t2.day THEN 1 \n",
        "  |    ELSE 0\n",
        "  |END AS is_holiday \n",
        " |FROM holy_int_weekday t2 \n",
        " |RIGHT JOIN jam_int_weekday t1  \n",
        " |on t1.month = t2.month and t1.day = t2.day\"\"\")\n",
        " */\n",
        " \n",
        "val csv = jam_int_weekday.join(holy_int_weekday, jam_int_weekday(\"month\") === holy_int_weekday(\"month\") && jam_int_weekday(\"day\") === holy_int_weekday(\"day\"), \"leftsemi\").select(jam_int_weekday(\"location_x\"), jam_int_weekday(\"location_y\"), jam_int_weekday(\"pub_millis\"), jam_int_weekday(\"level\"), jam_int_weekday(\"speed\"), jam_int_weekday(\"pub_date\"), jam_int_weekday(\"date_pst\"), jam_int_weekday(\"month\"), jam_int_weekday(\"day\"), jam_int_weekday(\"hour\"), jam_int_weekday(\"min\"),  jam_int_weekday(\"sec\"), jam_int_weekday(\"weekday\")).withColumn(\"is_holiday\", lit(1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "4zuV6Pa8zDLN"
      },
      "outputs": [],
      "source": [
        "val csv_neg = jam_int_weekday.join(holy_int_weekday, (jam_int_weekday(\"month\") !== holy_int_weekday(\"month\")) || (jam_int_weekday(\"day\") !== holy_int_weekday(\"day\")), \"leftsemi\").select(jam_int_weekday(\"location_x\"), jam_int_weekday(\"location_y\"), jam_int_weekday(\"pub_millis\"), jam_int_weekday(\"level\"), jam_int_weekday(\"speed\"), jam_int_weekday(\"pub_date\"), jam_int_weekday(\"date_pst\"), jam_int_weekday(\"month\"), jam_int_weekday(\"day\"), jam_int_weekday(\"hour\"), jam_int_weekday(\"min\"), jam_int_weekday(\"sec\"), jam_int_weekday(\"weekday\")).withColumn(\"is_holiday\", lit(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "IFbgFsUazDLN"
      },
      "outputs": [],
      "source": [
        "val csv_all = csv.union(csv_neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "N2arPZ7YzDLN"
      },
      "outputs": [],
      "source": [
        "csv_all.filter(\"is_holiday==0\").show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "YDkOE9J3zDLO"
      },
      "outputs": [],
      "source": [
        "csv_all.filter(\"is_holiday==1\").show(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "c6pYxQ8MzDLO"
      },
      "outputs": [],
      "source": [
        "val data = csv_all.select((col(\"location_x\").cast(\"Double\")),(col(\"location_y\").cast(\"Double\")), (col(\"pub_millis\").cast(\"Float\")), (col(\"month\").cast(\"Integer\")), (col(\"day\").cast(\"Integer\")), (col(\"hour\").cast(\"Integer\")), (col(\"min\").cast(\"Integer\")), (col(\"sec\").cast(\"Integer\")),(col(\"weekday\").cast(\"Integer\")),(col(\"is_holiday\").cast(\"Integer\")), (col(\"level\").cast(\"Integer\")),((col(\"level\") > 2).cast(\"Integer\").alias(\"label\")))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "lDee1SvkzDLO"
      },
      "outputs": [],
      "source": [
        "data.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "H2n3rfhizDLO"
      },
      "outputs": [],
      "source": [
        "val sqlTrans = new SQLTransformer().setStatement(\"\"\"SELECT location_x, location_y, SIN((weekday)*(2*PI()/7)) as sin_weekday, COS((weekday)*(2*PI()/7)) as cos_weekday, SIN((month-1)*(2*PI()/12)) as sin_month, COS((month-1)*(2*PI()/12)) as cos_month, SIN((day-1)*(2*PI()/31)) as sin_day, COS((day-1)*(2*PI()/31)) as cos_day, SIN(hour*(2*PI()/24)) as sin_hour, COS(hour*(2*PI()/24)) as cos_hour, SIN(min*(2*PI()/60)) as sin_min, COS(min*(2*PI()/60)) as cos_min , SIN(sec*(2*PI()/60)) as sin_sec, COS(sec*(2*PI()/60)) as cos_sec,\n",
        "   |CASE \n",
        "   | WHEN (hour+min/60) >= 7 and (hour+min/60) <= 9 THEN 1 \n",
        "   | WHEN (hour+min/60) >= 15 and (hour+min/60) <= 18 THEN 1 \n",
        "   | ELSE 0 \n",
        "   |END AS is_rush, \n",
        "   |CASE \n",
        "   |WHEN weekday >5 THEN 1\n",
        "   |ELSE 0\n",
        "  |END AS is_weekend, \n",
        "|is_holiday, level, label FROM __THIS__\"\"\")\n",
        "\n",
        "val dataTrans = sqlTrans.transform(data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKeCUCBUzDLP"
      },
      "source": [
        "## Insert the data set to the storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "QI42xtIxzDLP"
      },
      "outputs": [],
      "source": [
        "var pwd_folder = \"\"\n",
        "if (IS_HDFS){\n",
        "    pwd_folder = \"/user/jwoo5/\"\n",
        "} else {\n",
        "    pwd_folder = \"hdp-240/\"\n",
        "}\n",
        "val bucket_clean = pwd_folder + \"waze_clean/\"\n",
        "var loc_size = folder_name // \"/gb400m.parquet\"\n",
        "\n",
        "/*\n",
        "if(IS_40M){\n",
        "    //loc_size = \"/gb40m.parquet\"\n",
        "    loc_size = \"/gb400m.parquet\"\n",
        "} else {\n",
        "    //loc_size = \"/gb1.6.parquet\"\n",
        "    loc_size = \"/gb0.8.parquet\"\n",
        "}\n",
        "*/\n",
        "\n",
        "var full_path = \"\"\n",
        "/*\n",
        "var splits = if (!IS_FULL) dataTrans.randomSplit(Array(0.75, 0.25), seed=1234L) \n",
        "var train  = if (!IS_FULL) splits(0).withColumnRenamed(\"label\", \"trueLabel\")\n",
        "var test  = if (!IS_FULL) spark.emptyDataFrame splits(1).withColumnRenamed(\"label\", \"trueLabel\")\n",
        "*/\n",
        "var train_path = \"\"\n",
        "var test_path = \"\"\n",
        "\n",
        "var start = System.nanoTime\n",
        "\n",
        "/*\n",
        "if(IS_FULL){\n",
        "\tfull_path = \"gs://\"+ bucket_clean+loc_size+\"/full/\"\n",
        "    val full_writer = dataTrans.write.option(\"header\", true) //.schema(schema)\n",
        "    full_writer.format(\"parquet\").mode(\"overwrite\").save(full_path)\n",
        "} else {\n",
        "    var splits = dataTrans.randomSplit(Array(0.75, 0.25), seed=1234L)\n",
        "    var train = splits(0).withColumnRenamed(\"label\", \"trueLabel\")\n",
        "    var test = splits(1).withColumnRenamed(\"label\", \"trueLabel\")\n",
        "    train_path = \"gs://\"+ bucket_clean+ loc_size+\"/train/\"\n",
        "\ttest_path = \"gs://\"+ bucket_clean+loc_size+\"/test/\"\n",
        "\t\n",
        "    // Train data: df.write.format(source).mode(\"overwrite\").save(path)\n",
        "    val train_writer = train.write.option(\"header\", true) //.schema(schema)\n",
        "    train_writer.format(\"parquet\").mode(\"overwrite\").save(train_path)\n",
        "    \n",
        "    var end = System.nanoTime\n",
        "    print(end - start)\n",
        "    // Test data\n",
        "    val test_writer = test.write.option(\"header\", true) //.schema(schema)\n",
        "    test_writer.format(\"parquet\").mode(\"overwrite\").save(test_path)\n",
        "\n",
        "    end = System.nanoTime\n",
        "    print(end - start)\n",
        "}\t\t\t\t\t\t\t\n",
        "\n",
        "*/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "4LyERvK1zDLP"
      },
      "outputs": [],
      "source": [
        "if (IS_HDFS){\n",
        "    full_path = \"hdfs://\"+ bucket_clean+loc_size+\"/full/\"\n",
        "} else {\n",
        "    full_path = \"gs://\"+ bucket_clean+loc_size+\"/full/\"\n",
        "    \n",
        "}\n",
        "val full_writer = dataTrans.write.option(\"header\", true) //.schema(schema)\n",
        "full_writer.format(\"parquet\").mode(\"overwrite\").save(full_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "lqjZZPjczDLQ"
      },
      "outputs": [],
      "source": [
        "dataTrans.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "U4HKcNRDzDLQ"
      },
      "outputs": [],
      "source": [
        "//dataTrans.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": "auto",
        "id": "AiTQor59zDLR"
      },
      "outputs": [],
      "source": [
        "// Now lets actually process the data\\n\",\n",
        "var start = System.nanoTime\n",
        "/*\n",
        "var full_writer = null //.schema(schema)\n",
        "var train_writer = null\n",
        "var test_writer = null\n",
        "\n",
        "if(IS_FULL){\n",
        "    val full_writer = dataTrans.write.option(\"header\", true) //.schema(schema)\n",
        "    full_writer.format(\"parquet\").mode(\"overwrite\").save(full_path)\n",
        "    \n",
        "} \n",
        "*/"
      ]
    }
  ]
}